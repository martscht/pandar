---
title: ANCOVA
date: '2024-09-26'
slug: ancova-klinische
categories: ["KliPPs"]
tags: ["Regression", "ANCOVA", "Effektschätzer"]
subtitle: ''
summary: ''
authors: [schultze, nehler, irmer]
weight: 3
lastmod: '`r Sys.Date()`'
featured: no
banner:
  image: "/header/canceled.jpg"
  caption: "[Courtesy of pexels](https://www.pexels.com/photo/concrete-building-under-blue-sky-4004291/)"
projects: []
share: false

links:
  - icon_pack: fas
    icon: book
    name: Inhalte
    url: /lehre/klipps/ancova-klinische
  - icon_pack: fas
    icon: terminal
    name: Code
    url: /lehre/klipps/ancova-klinische.R
  - icon_pack: fas
    icon: newspaper
    name: Artikel
    url: https://doi.org/10.1038/s41562-021-01235-0
  - icon_pack: fas
    icon: folder-open
    name: OSF
    url: https://osf.io/8mk6x/
    
output:
  html_document:
    keep_md: true
---

```{r setup, cache = FALSE, include = FALSE, purl = FALSE}
# Aktuell sollen die global options für die Kompilierung auf den default Einstellungen gelassen werden
library(ggplot2) # ggplot2 und dplyr werden nur für Grafiken benötigt
source('https://pandar.netlify.app/lehre/statistik-ii/pandar_theme.R')
```

{{< toc >}}

## Einleitung

Die **AN**alysis of **COVA**riance hatten wir bereits im [2. Semester der Bachelorstudiengangs behandelt](lehre/statistik-ii/ancova-regression/#ancova) und dabei schon vorausahnen lassen, dass wir uns das Ganze im Rahmen des KliPPs-Masters noch einmal detaillierter angucken werden. Dieser Spannungsbogen soll nun hier abgeschlossen werden. 

Im Allgemeinenen wird die ANCOVA in der Psychotherapieforschung immer dann genutzt, wenn wir Interventionsgruppen betrachten werden, die sich zum ersten Zeitpunkt hinsichtlich des Outcomes oder beliebiger Kovariaten unterscheiden *können*. Hierbei ist besonders wichtig zu unterscheiden, ob es sich um randomisierte Studien (üblicherweise "Randomized Controlled Trial", kurz RCT, genannt) handelt oder nicht. Üblicherweise sollten sich die Gruppen im ersten Fall nicht systematisch unterscheiden, aber wir werden im Abschnitt zum [Dropout](#dropout) noch genauer darauf eingehen, wie so etwas trotzdem entstehen kann. Darüber hinaus werden wir einige kritische Entscheidungen und Abgrenzungen besprechen, z.B. wann es empfehlenswert ist als abhängige Variable die Differenz aus den zwei Messzeitpunkten (also den Indikator der Veränderung) statt des Messwerts zum 2. Zeitpunkt zu nutzen.

### Interventionsstudie zur Depression während der COVID Pandemie

Auch wenn dieser Beitrag etwas theorielastiger wird, als die vergangenen Abschnitte, werden wir uns auch dieses mal hauptsächlich an einer spezifische klinischen Studie orientieren. Die Basis für unsere Auswertungen liefert eine Studie von [Schleider et al. (2022)](https://doi.org/10.1038/s41562-021-01235-0). Anders als bei den bisherigen Beiträgen muss ich allerdings von vornherein zugeben, dass wir die Ergebnisse nicht exakt reproduzieren werden - und das hier auch nicht direkt anpeilen werden - weil in der Studie mitunter etwas aufwändigere Datenauf- und -vorbereitungsschritte durchgeführt wurden, die uns hier etwas zu weit vom eigentlichen Thema ablenken würden.

Wie immer können Sie die Originaldaten direkt aus dem [OSF Repo](https://osf.io/8mk6x/) beziehen. Die relevante Auswahl aus dem sehr großen Datensatz habe ich schon einmal vorbereitet und hier hinterlegt:

```{r, eval = FALSE}
source('https://pandar.netlify.app/daten/Data_Processing_cope.R')
```
```{r, echo = FALSE, purl = FALSE}
source('../../daten/Data_Processing_cope.R')
```

Im Datensatz sind insgesamt `r ncol(cope)` Variablen enthalten, von denen zwar nicht alle zentral für die ANCOVA sind, aber zumindest in den Tabellen der Studie aufbereitet werden. Im Artikel werden die Ergebnisse einer großen Interventionsstudie vorgestellt, in der die Effektivität von zwei selbst-administrierten single-session interventions (SSI) mit Hinblick auf die Reduktion von depressiven Symptomen bei Jugendlichen während der COVID-19 Pandemie untersucht wurde. Die Studie ist im Journal [Nature Human Behavior](https://www.nature.com/nathumbehav), erschienen und gehört mit seinen [126 Zitationen](https://www.webofscience.com/wos/woscc/full-record/WOS:000728447200001) zum 1% der am häufigst zitierten psychologischen Publikationen des Jahres 2022. 

<details><summary><b>Variablenübersicht</b></summary>

Variable | Beschreibung | Ausprägungen
--- | ------ | ----- 
`condition` | Experimentalbedingung | `Placebo Control`, `Project Personality`, `Project ABC`
`race_...` | Verschiedene Angaben zur Ethnizität | `0` = identifiziert sich nicht als, `1` = identifiziert sich als
`gender_...` | Verschiedene Angaben zum Geschlecht | `0` = identifiziert sich nicht als, `1` = identifiziert sich als
`sex` | Geburtsgeschlecht | `Female`, `Male`, `Other`, `Prefer not to say`
`orientation` | Sexuelle Orientierung | `Asexual`, `Bisexual`, `Gay/Lesbian/Homosexual`, `Heterosexual/Straight`, `I do not use a label`, `I do not want to respond`, `Other/Not listed (please specify)`, `Pansexual`, `Queer`, `Unsure/Questioning`
`age` | Alter | Alter in Jahren
`CDI1` | Children's Depression Inventory, Baseline | _Skalenmittelwert_, 0-2
`CDI3` | Children's Depression Inventory, Follow-up | _Skalenmittelwert_, 0-2
`CTS1` | COVID Trauma Symptoms, Baseline | _Skalenmittelwert_, 1-4
`CTS3` | COVID Trauma Symptoms, Follow-up | _Skalenmittelwert_, 1-4
`GAD1` | Generalized Anxiety Disorder, Baseline | _Skalenmittelwert_, 1-4
`GAD3` | Generalized Anxiety Disorder, Follow-up | _Skalenmittelwert_, 1-4
`SHS1` | State Hope Scale, Perceived Agency Subscale, Baseline | _Skalenmittelwert_, 1-8
`SHS2` | State Hope Scale, Perceived Agency Subscale, Post-Intervention | _Skalenmittelwert_, 1-8
`SHS3` | State Hope Scale, Perceived Agency Subscale, Follow-up | _Skalenmittelwert_, 1-8
`BHS1` | Beck Hopelessness Scale, Baseline | _Skalenmittelwert_, 0-3
`BHS2` | Beck Hopelessness Scale, Post-Intervention | _Skalenmittelwert_, 0-3
`BHS3` | Beck Hopelessness Scale, Follow-up | _Skalenmittelwert_, 0-3
`DRS1` | Dietary Restriction Screener, Baseline | _Skalenmittelwert_, 0-1
`DRS3` | Dietary Restriction Screener, Follow-up | _Skalenmittelwert_, 0-1
`pfs_1` | Program Feedback Scale, "Enjoyed" | 1-5
`pfs_2` | Program Feedback Scale, "Understood" | 1-5
`pfs_3` | Program Feedback Scale, "Easy to Use" | 1-5
`pfs_4` | Program Feedback Scale, "Tried My Hardest" | 1-5
`pfs_5` | Program Feedback Scale, "Helpful to Other Kids" | 1-5
`pfs_6` | Program Feedback Scale, "Would Recommend to a Friend" | 1-5
`pfs_7` | Program Feedback Scale, "Agree with Message" | 1-5
`dropout1` | Abbruch der Teilnahme, Post-Intervention | `0` = nicht abgebrochen, `1` = abgebrochen
`dropout2` | Abbruch der Teilnahme, Follow-Up | `0` = nicht abgebrochen, `1` = abgebrochen


</details>

### Deskriptivstatistik

Wie schon in den anderen Beiträgen, können wir zunächst checken, ob unsere Daten mit denen aus dem Artikel übereinstimmen, indem wir die deskriptivstatistischen Tabellen nachvollziehen. Tabelle 1 (S. 260) gibt dabei eine sehr detaillierte Übersicht über die die Verteilung der demografischen Variablen Ethnizität, Geschlecht und sexueller Orientierung. Diese Ergebnisse sind mit dem bereitgestellten Datensatz soweit reproduzierbar - allerdings aufgrund der fehlenden Exklusivität der Kategorien nicht so einfach zu erstellen, wie in den vergangenen beiden Beiträgen:

```{r}
# Ethnizität
ethnicity <- aggregate(cope[, grep('race_', names(cope))], by = list(cope$condition), FUN = sum)
names(ethnicity) <- c('condition', 'Native', 'Asian', 'Hispanic', 'Pacific Islander', 'White', 'Black', 'Other', 'Prefer not to say')
ethnicity

# Geschlecht
gender <- aggregate(cope[, grep('gender_', names(cope))], by = list(cope$condition), FUN = sum)
names(gender) <- c('condition', 'agender', 'not sure', 'other', 'androgynous', 'nonbinary', 'two spirited', 'Trangender female to male', 'trans female', 'trans male', 'Gender expansive', 'Third gender', 'Genderqueer', 'Transgender male to female', 'Man', 'Woman')
gender
```

Für das biologische Geschlecht und die sexuelle Orientierung können wir die Ergebnisse mittels einfacher Häufigkeitstabeller erzeugen:
```{r}
# Biologisches Geschlecht
table(cope$sex, cope$condition)

# Sexuelle Orientierung
table(cope$orientation, cope$condition)
```

Und zu guter Letzt noch die mittlere Ausprägung des Primäroutcomes (Depressive Symptomatik) getrennt nach Interventionsgruppe zur Baseline. Dabei ist zu bedenken, dass die Variable im Datensatz als Mittelwert vorliegt (und auch in späteren Berechnungen so verwendet wird), in dieser Tabelle allerdings als Summenscore angegeben wird. Daher müssen wir den Mittelwert wieder mit der Anzahl der Items (12) multipliuieren:

```{r}
# Baseline CDI
psych::describeBy(cope$CDI1*12, cope$condition)
```

Im Artikel präsentieren [Schleider et al. (2022)](https://doi.org/10.1038/s41562-021-01235-0) in Tabelle 2 außerdem die Akzeptanz bzw. Zufriedenheit mit der jeweiligen Intervention, die wir ebenfalls sehr leicht rekonstruieren können: 

```{r}
# Akzeptanz
pfs <- subset(cope, select = c('condition', grep('pfs_', names(cope), value = TRUE))) |> na.omit()
psych::describeBy(pfs, pfs$condition)

```


## Dropout

Schwieriger als im letzten Abschnitt wird es, wenn wir versuchen die Werte in Tabelle 3 des Artikels zu reproduzieren. Das liegt an folgendem Vorgehen, das auf S. 266 von [Schleider et al. (2022)](https://doi.org/10.1038/s41562-021-01235-0) beschrieben wird:

> We imputed participant-level missing data using the expectation-maximization and bootstrapping algorithm implemented with Amelia II in R, allowing more conservative intent-to-treat analyses than other approaches

Was hier beschrieben wird, ist der Umgang mit einem zentralen Problem in jeder Form längsschnittlicher Datenerhebung, das aber in der Psychotherapieforschung besonders relevant ist: dem Studienabbruch von Teilnehemden. Generell können wir niemals davon ausgehen, dass alle Personen, die zum ersten Zeitpunkt an einer Studie teilnehmen, dieser auch langfristig erhalten bleiben. Insbesondere, wenn die Studienteilnahme mit Anstrengungen oder Zeitaufwand verbunden ist (was quasi unumgänglich ist) oder wir z.B. Personen mit psychischen Störungen untersuchen, ist die Rate an Studienabbrüchen oft sehr hoch. [Cooper und Conklin (2015)](https://doi.org/10.1016/j.cpr.2015.05.001) zeigen in einer Meta-Analyse eine Drop-Out Rate von 17,5% bis 20% (wobei einzelne Studien von 0% bis 50% reichen) in klinischen RCT Studien. Generell ist es natürlich "schade" wenn Personen zu späteren Zeitpunkten nicht mehr an der Studie teilnehmen, weil wir so für Veränderungshypothesen weniger Power haben, aber das eigentliche Problem stellt sogenannter _differentieller Dropout_ dar. Damit ist gemeint, dass Personen aus unterschiedlichen Experimentalgruppen in unterschiedlichem Umfang und aus unterschiedlichen Gründen die Teilnahme an der Studie abbrechen. Das kann dazu führen, dass die Gruppen sich in ihrer Zusammensetzung verändern und damit die schönen Gruppeneigenschaften, für die wir uns extra ein Randomisierungsschema ausgedacht haben, wieder verloren gehen. So kann es passieren, dass Gruppen, die zwar zur Baseline noch direkt vergleichbar waren, im Verlauf der Zeit immer unterschiedlicher werden.

Der erste logische Schritt mit diesem Problem umzugehen ist es, festzustellen in welchem Ausmaß der Dropout über die Gruppen hinweg unterschiedlich ist. Im Artikel von [Schleider et al. (2022)](https://doi.org/10.1038/s41562-021-01235-0) wird auf S. 259 genau das getan. Um Häufigkeitsverteilung zwischen mehreren Gruppen zu vergleichen, wird üblicherweise der $\chi^2$ herangezogen:

```{r}
dropouts <- table(cope$condition, cope$dropout1)
dropouts
prop.table(dropouts, margin = 1)
chisq.test(dropouts)
```
Wir sehen hier die Ergebnisse, die auch von  [Schleider et al. (2022)](https://doi.org/10.1038/s41562-021-01235-0) berichtet werden: es gibt einen statistisch bedeutsamen Unterschied in den Häufigkeiten des Teilnahmeabbruchs über die Gruppen hinweg ($\chi^2 = `r round(chisq.test(dropouts)$statistic, 2)`$, $df = 2$, $p < .001$) und zwar in der Form, dass im Project ABC deutlich seltener abgebrochen wird, als in den anderen beiden Gruppen. Diese unterschiedliche Quote des Abbruchs kann kausale Schlüsse hinsichtlich der Interventionen gefährden. Nähere Informationen dazu, wie wir bessere kausale Effektschätzer ermitteln können und was wir dabei bedenken müssen, finden Sie unter anderem in den beiden Beiträgen zur Kausaleffektschätzung mit [EffectLite](/lehre/klipps-legacy/kausaleffekte1-legacy/) bzw. [Propensity-Scores](/lehre/klipps-legacy/kausaleffekte2-legacy/).

Bevor wir untersuchen, ob die Tendenz zum Dropout auch mit anderen relevanten Variablen zusammenhängt, an dieser Stelle ein extrem kurzer Überblick über die drei Kategorien, anhand derer fehlende Werte statistisch unterschieden werden:

Typ | Abkürzung | Implikation
--- | -- | -------
Missing Completely at Random | MCAR | Fehlende Werte sind unabhängig von den beobachteten und nicht beobachteten Werten
Missing at Random | MAR | Fehlende Werte sind abhängig von den beobachteten Werten, aber nicht von den nicht beobachteten Werten
Missing Not at Random | MNAR | Fehlende Werte sind abhängig von den nicht beobachteten Werten

Was mit der Abhängigkeit von beobachteten bzw. nicht-beobachteten Werten gemeint ist, wir am besten mit ein paar Beispielen deutlich. MCAR sind üblicherweise fehlende Werte, die durch den Versuchsaufbau gezielt herbei geführt werden. Wenn wir zum Beispiel an extrem vielen Konstrukten interessiert sind, aber unsere Studienteilnehmenden nicht alle durch eine 4-stündige Erhebung quälen wollen, können wir die Personen randomisiert in Gruppen zuweisen, die jeweils unterschiedliche Kombinationen von Fragebögen in der Erhebung durchlaufen müssen. In diesem Fall hängt das Fehlen der Werte ausschließlich von dieser zufälligen Einteilung ab. Wenn wir zum Beispiel eine Erhebung mit Kindern in einer Schulstunde durchführen und die Befragung ziemlich genau auf die 45 Minuten angelegt ist, kann es vorkommen, dass einige Kinder aufgrund einer Dyslexie die Fragebögen sehr viel langsamer bearbeiten und so deren Werte auf den letzten Skalen fehlen. In diesem Fall liegt das Fehlen der Werte an der Dyslexie, also einer anderen Variablen, die wir (idealerweise) bereits zuvor irgendwo erfasst haben. Solche Fälle bezeichnet man als MAR (missing at random), weil sie nach der Kontrolle auf diese Störvariablen zufällig fehlen. Missing not at random (MNAR) sind Werte dann, wenn sie fehlen, weil Personen auf der spezifische Variable eine besondere Ausprägung haben. So kann es sein, dass Personen sich weigern Fragen zu ihrem Alkoholkonsum zu beantworten, wenn sie diesen selbst bereits als problematisch wahrnehmen und daher versuchen, ihn zu verbergen. Da in diesen Fällen das Fehlen von Werten von den Werten abhängt, die wir _nicht_ beobachten konnten, ist es hier sehr viel schwieriger das Problem fehlender Werte nach der Erhebung mit statistischen Modellen zu lösen. Gerade in der Psychotherapieforschung ist es aber nicht unwahrscheinlich, dass Personen die Studienteilnahme abbrechen, weil sie die Intervention nicht als wirksam erleben oder z.B. eine so schwere Episode erleiden, dass sie stationär behandelt werden müssen und somit nicht mehr an der Erhebung in der Ambulanz teilnehmen können.

### Prädiktoren des Dropouts

Wie Sie aus diesem (etwas zu lang geratenen) Absatz vermutlich ableiten können, sollten Entscheidungen zur Behandlung fehlender Werte vor allem auf inhaltlichen Überlegungen fußen. Statistisch können wir zumindest prüfen, ob die Variablen, die wir zum 1. Messzeitpunkt erhoben haben, prädiktiv für einen Teilnahmeabbruch sind und demzufolge die inhaltlichen und kausalen Schlussfolgerungen hinsichtlich der Interventionseffektivität gefährden würden. Dazu können wir im einfachsten Fall die logistische Regression, die wir im [letzten Beitrag](/lehre/klipps/logistische-regression-klinische) erarbeiteten hatten, nutzen, um das Fehlen von Werten vorherzusagen.

Für den Dropout während der Intervention gibt es die Variable `dropout1` als Indikator. Auf dieser Variable ist kodiert, ob Teilnehmende die 1. Frage des Fragebogen direkt nach Abschluss der Intervention ausgefüllt haben. In logistischer Regression können wir diese also relativ einfach durch die Baselinevariablen vorhersagen, um zu sehen, ob es spezifische Prädiktoren für den Abbruch gibt. Damit ich die Namen der Prädiktoren nicht alle einzeln hinschreiben muss, erzeuge ich mir erst einen Datensatz, der nur die relevanten Variablen enthält und nutze dann alle darin enthaltenen Variablen als Prädiktoren.

```{r}
# Datenauswahl zur Dropout-Vorhersage
drop_dat <- cope[, c('dropout1', grep('^gender', names(cope), value = TRUE),
    grep('^race', names(cope), value = TRUE),
    'age', 'CDI1', 'CTS1', 'GAD1', 'SHS1', 'BHS1', 'DRS1')]

# Vorhersage von Dropout mit logistischer Regression
drop_mod <- glm(dropout1 ~ ., drop_dat, family = 'binomial')
```

<details><summary><b>Die etwas lange Ergebnistabelle</b></summary>
```{r}
summary(drop_mod)
```
</details>

```{r, echo=FALSE, purl = FALSE}
tmp <- summary(drop_mod)$coefficients
tmp <- min(p.adjust(tmp[, 'Pr(>|z|)'], 'holm'))
```


Von den `r length(coef(drop_mod))` Tests der Regressionsgewichte sind insgesamt 2 statistisch bedeutsam, was zunächst nicht vielsagend wirkt (wenn man die 5% Irrtumswahrscheinlichkeit bedenkt). Oder etwas wissenschaftlicher ausgedrückt: nach Bonferroni-Holm Adjustierung liegt der niedrigste $p$-Wert der Regressionsgewichte bei `r round(tmp, 3)`. Auch die Confusion-Matrix, die wir [im letzten Beitrag](/lehre/klipps/logistische-regression-klinische#klassifikationsgute) genutzt hatten, um die Güte des Modells genauer zu beurteilen, deutet darauf hin, dass wir nicht besonders gut in der Lage sind, den Dropout anhand der Baselinevariablen vorherzusagen (weil wir für beinahe alle Personen vorhersagen, dass sie zum 2. Zeitpunkt teilnehmen):

```{r}
# Vorgesagte Werte
cope$dropout_pred <- predict(drop_mod, cope, type = 'response') > .5
cope$dropout_pred <- factor(cope$dropout_pred, labels = c('remain', 'dropout'))

# Confusion Matrix
caret::confusionMatrix(cope$dropout_pred, cope$dropout1)
```

### Umgang mit Dropouts 

Dass sich Abbrecher*innen hinsichtlich ihrer Baselineeigenschaften kaum von Personen unterscheiden, die zum 2. Zeitpunkt teilgenommen haben, bietet jedoch keine Sicherheit, dass die Dropouts tatsächlich MCAR sind. Genausogut könnten die fehlenden Werte davon abhängen, dass die Personen die Intervention nicht gut fanden (und dementsprechend andere Effekte der Interventionen für sie gelten) oder durch andere Baseline-Kovariaten gesteuert sein, die einfach nicht erhoben wurden (z.B. Gewissenhaftigkeit). Üblicherweise werden daher in der Psychotherapieforschung mehrere Schätzer der Effekte bestimmt, welche eine Einengung des tatsächlichen Effekts erlauben sollen. Als "Obergrenze" wird dabei die Schätzung angesehen, die aus der Analyse der vollständigen Beobachtungen (sogenannte _Complete Case Analysis_, CCA) hervorgeht. Die Annahme ist dabei, dass Personen deren Outcomes besonders gut sind, dem Treatment treu bleiben. Dem gegenüber kann als "Untergrenze" die Analyse aller Personen angesehen werden, die mit dem Treatment begonnen haben (sogennantes _Intent-to-Treat_, ITT). Dabei stellt sich natürlich die Frage, welche Werte genutzt werden sollen, um die fehlenden Werte zu ersetzen. Eine der klassischen Methoden ist es, die zuletzt gemachte Beobachtung zu kopieren und somit für die Personen, die abgebrochen haben, anzunehmen, dass sie sich nicht verändert hätten. In der Psychotherapieforschung wird diese Methode oft als _Last Observation Carried Forward_ (LOCF) bezeichnet. Neben diesen eher klassischen Methoden gibt es außerdem eine Vielzahl von modernen statistischen Verfahren, welche verwendet werden können, um bessere Schätzungen dieser fehlenden Werte zu erhalten. Im Artikel von [Schleider et al. (2022)](https://doi.org/10.1038/s41562-021-01235-0) wird die multiple Imputation verwendet, die in R zum Beispiel im Paket `Amelia` oder `mice` implementiert ist. Andere Methoden sind zum Beispiel die Nutzung von Propensity scores (wie sie im [dazugehörigen Beitrag](/lehre/klipps-legacy/kausaleffekte2-legacy/) vorgestellt werden) oder generalized estimating equations.

Hier werden wir uns erst einmal auf die klassischen Methoden beschränken und CCA und LOCF nutzen, um eine grobe Vorstellung von den Interventionseffekten zu bekommen. Die Umsetzung mit Amelia, die im Artikel von [Schleider et al. (2022)](https://doi.org/10.1038/s41562-021-01235-0) durchgeführt wurde ist aber im entsprechenden [Skript im OSF-Repo](https://osf.io/uhzdx) zu finden. Um LOCF auch umsetzen zu können, müssen wir einen Datensatz erstellen, in dem alle fehlenden Werte durch den vorangegangenen Wert ersetzt sind:

```{r}
locf <- cope

# LOCF, post-intervention
locf$SHS2 <- ifelse(is.na(locf$SHS2), locf$SHS1, locf$SHS2)
locf$BHS2 <- ifelse(is.na(locf$BHS2), locf$BHS1, locf$BHS2)

# LOCF, follow-up
locf$CDI3 <- ifelse(is.na(locf$CDI3), locf$CDI1, locf$CDI3)
locf$CTS3 <- ifelse(is.na(locf$CTS3), locf$CTS1, locf$CTS3)
locf$GAD3 <- ifelse(is.na(locf$GAD3), locf$GAD1, locf$GAD3)
locf$SHS3 <- ifelse(is.na(locf$SHS3), locf$SHS2, locf$SHS3)
locf$BHS3 <- ifelse(is.na(locf$BHS3), locf$BHS2, locf$BHS3)
locf$DRS3 <- ifelse(is.na(locf$DRS3), locf$DRS1, locf$DRS3)
```

Hier nutzen wir `ifelse`, um zu prüfen, ob auf einer Variable ein fehlender Wert vorliegt. Wenn dem so ist (zweites Argument) wird der Wert durch den Wert der vorherigen Messung ersetzt, ansonsten durch den beobachteten Werte (drittes Argument).

## ANCOVA

Nachdem wir uns nun ausführlich mit dem Problem des Dropouts beschäftigt haben, kommen wir zum eigentlich Punkt dieses Beitrags zurück. Mit der ANCOVA können wir Gruppenunterschiede testen, während wir gleichzeitig auf Kovariaten kontrollieren. Häufig ist dabei der Wert des Outcomes zur Baseline die zentrale Kovariate, aber letztlich ist jede Kovariate dabei denkbar. Im Artikel von [Schleider et al. (2022)](https://doi.org/10.1038/s41562-021-01235-0) werden anhand verschiedener ANCOVAs die Effekt der Intervention auf dem Primär- und allen Sekundäroutcomes überprüft. Die Ergebnisse werden allerdings nicht im Artikel selbst, sondern in der [Supplementary Information](https://static-content.springer.com/esm/art%3A10.1038%2Fs41562-021-01235-0/MediaObjects/41562_2021_1235_MOESM1_ESM.docx) berichtet.

Wir können uns hier zunächst auf das Primäroutcome konzentrieren und werden dabei auch ein wenig vom Vorgehen von Schleider et al. (2022) abweichen. Zum Einen haben wir nicht mit der Multiple Imputation gearbeitet, zum Anderen werden wir auch noch die _generalisierte_ ANCOVA nutzen, um zu untersuchen, ob die unterschiedlichen Interventionen differentielle Effekte aufweisen. Zunächst aber die klassische ANCOVA.

Im Artikel wird die ANCOVA genutzt, um sicherzustellen, dass die Unterschiede zwischen den Gruppen _bedingt auf_ die Werte zum ersten Zeitpunkt interpretiert werden können. Gucken wir uns am besten genauer an, was ich damit meine:

```{r}
# ANOVA model
mod1 <- lm(CDI3 ~ condition, cope)

# Ergebniszusammenfassung
summary(mod1)
```
Im klassischen ANOVA-Modell als Prädiktor genutzt, um die abhängige Variable vorherzusagen. Die jeweilige Regressionsgewichte sind also die _mittleren_ Gruppenunterschiede (zur Äquivalenz von ANOVA und Regression gibt es [hier noch einen expliziten Beitrag](/lehre/klipps-legacy/anova-regression-legacy)). Das heißt, wenn wir die Gruppenmittelwerte des CDI zur Baseline betrachten:

```{r}
# Gruppenmittelwerte zur Baseline
tapply(cope$CDI1, cope$condition, mean)
```
dass wir Personen aus der Kontrollgruppe nutzen, die einen `CDI1`-Wert von $1.19$ hatten, wärend die Personen aus Project ABC einen Wert von $1.18$ hatten. Dieser Unterschied mag numerisch klein wirken, aber wir vergleichen eben (aufgrund des Dropouts) _leicht_ unterschiedliche Personen miteinander. In der ANCOVA ziehen wir zusätzlich den CDI-Wert aus der Baselinebefragung als Prädiktor heran:

```{r}
# Einfache ANCOVA
mod2 <- lm(CDI3 ~ CDI1 + condition, cope)

# Ergebniszusammenfassung
summary(mod2)
```
Der deutliche Sprung im $R^2$ weist darauf hin, dass die Depressionsymptomatik über die Zeit deutlich korreliert ist. Wie immer in der multiplen Regression ist der Effekt der Prädiktoren jetzt unter der Bedingungen zu interpretieren, dass die Werte auf den anderen Prädiktoren konstant sind. Heißt in unserem Fall, dass wir hier nun im Regressionsgewicht von `Project Personality` den Unterschied zwischen Personen aus den beiden Gruppen sehen, die mit dem gleichen BDI zur Baseline gestartet sind. Als Bild ausgedrückt:

```{r anova-vs-ancova, echo = FALSE, fig = TRUE, warning = FALSE}
# Regressionsergebnisse zusammenführen
lines <- data.frame(condition = c("Placebo Control", "Project Personality", "Project ABC"), 
  int1 = coef(mod1)[1] + c(0,coef(mod1)[2:3]),
  slo1 = 0,
  int2 = coef(mod2)[1] + c(0,coef(mod2)[3:4]),
  slo2 = coef(mod2)[2],
  mns = tapply(cope$CDI1, cope$condition, mean))

# Grafische Darstellung
scatter <- ggplot(cope, aes(x = CDI1, y = CDI3, color = condition)) + 
  theme_pandar() + scale_color_pandar() + geom_point(alpha = 0) + xlim(0.75, 1.25) + ylim(.6, 1.2) 

# Regressionsgeraden
scatter +
  geom_abline(data = lines, aes(intercept = int2, slope = slo2, color = condition), linetype = 'solid') +
  geom_point(data = lines, aes(x = mns, y = mns*slo2+int2))
```
In der ANCOVA ist der Abstand zwischen den _bedingten_ Gruppenmittelwerten über alle Ausprägungen von `CDI1` der Gleiche - wir können den Abstand der Linien an jeden Punkt bestimmen und es sind immer die Regressionsgewichte aus `mod2`. In der ANOVA hingegen haben wir konkret die drei eingezeichneten Punkte verglichen, die sich nicht nur deswegen unterscheiden, weil die Linien (die bedingten Gruppenmittelwerte) um $b_2$ bzw. $b_3$ versetzt sind, sondern auch, weil wir an unterschiedlichen Stellen der x-Achse nachgucken. In diesem Fall ist der Unterschied zwischen beiden zwar minimal (weil es sich um einen immens großen RCT handelt), aber prinzipiell könnten wir so auf diese Unterschiede kontrollieren.

Im Abgleich der Wege die fehlenden Werte zu behandeln, können wir die ANCOVA auch noch einmal für den `locf` Datensatz durchführen:

```{r}
# Modell mit Last-Observation-Carried-Forward
mod2_locf <- lm(CDI3 ~ CDI1 + condition, locf)

# Ergebniszusammenfassung
summary(mod2_locf)
```
Oder für die optischeren unter uns:

```{r ancova-locf, echo = FALSE, fig = TRUE, warning = FALSE}
# Regressionsergebnisse zusammenführen
lines$int3 <- coef(mod2_locf)[1] + c(0,coef(mod2_locf)[3:4])
lines$slo3 <- coef(mod2_locf)[2]

# Grafische Darstellung
scatter +
  geom_abline(data = lines, aes(intercept = int2, slope = slo2, color = condition), linetype = 'solid') +
  geom_abline(data = lines, aes(intercept = int3, slope = slo3, color = condition), linetype = 'dashed') 

```
Es lässt sich sofort erkennen, dass im LOCF-Ansatz der Zusammenhang zwischen `CDI1` und `CDI3` deutlich größer wird. Das überrascht wenig, wenn man bedenkt, dass wir für einige Personen den Werte aus der ersten einfach in die zweite Variable kopiert haben. Eine Überschätzung der Stabilität liegt also in der Natur der Sache. Es wird außerdem deutlich, dass der Effekt der Interventionen deutlich kleiner geworden ist, weil wir für die fehlenden Personen eine Veränderung von 0 angenommen haben. Die Schlussfolgerungen bleiben dennoch erhalten: beide Interventionen haben einen statistisch bedeutsamen Effekt über die Plazebo-Kontrollbedingung hinaus.

### Change-Modelle

Eine naheliegende Alternative zu den ANCOVA Modellen ist es, statt des zweiten Zeitpunkts die Veränderung zwischen den Zeitpunkten $Y_2 - Y_1$ durch Kovariaten vorherzusagen. Mit dem Vergleich dieser, sogenannten "Change-Modelle" mit der ANCOVA haben sich diverse Studien und methodische Arbeiten befasst. Ein klassisches Argument gegen die ANCOVA ist [Lord's Paradox](https://arxiv.org/pdf/2302.01822), in dem es zu unterschiedlichen Ergebnissen kommt, je nachdem ob man eine Veränderung mit einer ANCOVA oder einem Veränderungsmodell analysiert, wenn die Gruppen sich zum 1. Messzeitpunkt unterscheiden.

<details><summary><b>Lord's Paradox</b></summary>

Im ursprünglichen Kommentar von [Lord (1967)](https://psycnet.apa.org/doi/10.1037/h0025105) wird eine Situation beschrieben, in der eine Universität ihr Mensa-Essen evaluieren möchte. Dazu wird das Gewicht von Männern und Frauen zu Beginn des Wintersemesters und zum Ende des folgenden Sommersemesters erhoben. Nehmen wir als extremes Beispiel, dass beide Gruppen im Mittel 5kg zugelegt hätten:

```{r}
# Seed festlegen
set.seed(123)

# Werte Frauen
y1f <- rnorm(100, 70, 10)
y2f <- 5 + .5*y1f + rnorm(100, 35, sqrt(75))

# Werte Männer
y1m <- rnorm(100, 80, 10)
y2m <- 5 + .5*y1m + rnorm(100, 40, sqrt(75))

# Datensatz
d <- data.frame(y1 = c(y1f, y1m), y2 = c(y2f, y2m),
  g = rep(c('f', 'm'), each = 100))
```

In den Termini des bisherigen Beitrags ist das Geschlecht die Gruppen-Zuweisung. Wenn wir also untersuchen wollen, ob es einen Effekt der Variable Geschlecht gibt und dabei eventuelle Unterschiede in der Baseline berücksichtigen wollen (die es ja gibt), finden wir einen signifikanten Effekt:

```{r}
# ANCOVA
lord_ancova <- lm(y2 ~ y1 + g, d)
summary(lord_ancova)
```
Die ANCOVA würde uns also zurückmelden, dass das Geschlecht selbst nach Kontrolle auf die Baselineunterschiede einen signifikanten Effekt auf die Gewichtszunahme hat. Wenn wir aber die Veränderung zwischen den Zeitpunkten betrachten, finden wir keinen signifikanten Effekt:

```{r}
# Change-Modell
lord_change <- lm(y2 - y1 ~ g, d)
summary(lord_change)
```
Frauen verändern sich bedeutsam (im Intercept dargestellt) und die Veränderung der Männer unterscheidet sich nicht bedeutsam (im Regressionsgewicht dargestellt). Dieses Artefakt wird seither als Lord's Paradox bezeichnet.

</details>

Wenn wir einen Change-Ansatz nutzen möchten, um die Effekte der Interventionen auf das primäre Outcome (den CDI Depressionsscore) zu untersuchen, sieht das relativ einfach so aus:

```{r}
# Change-Modell
mod3 <- lm(CDI3 - CDI1 ~ condition, cope)

# Ergebniszusammenfassung
summary(mod3)
```
Numerisch sind die Effekte der beiden Gruppen beinahe identisch. Allerdings ist die inhaltliche Interpretation ein wenig unterschiedlich: in der ANCOVA war die Bedeutung des Regressionsgewichts von `Project Personality`, dass es den Unterschied zwischen Personen aus der Kontrollgruppe und dieser Gruppe zum zweiten Zeitpunkt beschreibt, wenn sie zum ersten Zeitpunkt den gleichen CDI Wert hatten. Im Change-Modell beschreibt das Regressionsgewicht hingegen den Unterschied zwischen zwei Personen aus den beiden Gruppen hinsichtlich ihrer Veränderung zwischen dem 1. und dem 2. Messzeitpunkt. Wenn wir dieses Modell jetzt in eine ANOVA überführen:

```{r}
# Change zur ANOVA
anova(mod3)
```

Entspricht der Test der `condition` genau dem Test, den wir für den Interaktionseffekt in der Messwiederholten ANOVA erhalten hätten. (Probieren Sie es gerne mit der [Messwiederholten ANOVA](/lehre/statistik-ii/anova-iii) aus!) Letztlich ist die Entscheidung zwischen Change-Model oder ANCOVA also die gleiche wie die zwischen Messwiederholter ANOVA und ANCOVA.

Wenn Sie eine Interventionsstudie durchführen, für welchen der beiden Ansätze sollten Sie sich jetzt also Entscheiden? Generell gibt es ein paar handfeste Empfehlungen: [van Breukelen (2013)](https://www.tandfonline.com/doi/abs/10.1080/00273171.2013.831743) konnte zeigen, dass beide Ansätze zu den gleichen Ergebnissen führen, wenn die Gruppen randomisiert zugewiesen werden, in solchen Fällen aber die ANCOVA mehr Power hat. Wenn "natürliche" Gruppen verglichen werden, die sich in der Variable vor der Intervention unterscheiden können und die Kovariaten mit Messfehler behaftet sind, wird generell empfohlen den Change-Ansatz zu verwenden. [Lüdtke und Robitzsch (2023)](https://doi.org/10.1080/00220973.2023.2246187) zeigen darüber hinaus, dass der Change-Ansatz dann genutzt werden sollte, wenn nicht alle potentiellen Kovariaten erhoben wurden, weil es in diesem Ansatz (unter der Annahme, dass die Konfundierung zu allen Zeitpunkten identisch ist) durch die Bildung der Differenz nicht notwendig ist, alle Kovariaten in das Modell aufzunehmen. Allerdings wird dabei Vorausgesetzt, dass das Outcome zum 1. Messzeipunkt keine Auswirkung auf die Gruppenzuordnung hat. In Fällen, in denen vorab eine sinnvolle und stichhaltige Erhebung potentieller Störvariablen geplant wurde, ist die Nutzung der ANCOVA empfehlenswert, sofern sie nicht aufgrund von Baselineunterschieden in Variablen mit fehlenden Werte zu potentiellen Problemen des Lord's Paradox führt.

## Generalisierte ANCOVA

Die oben vorgestellte ANCOVA lässt sich nach Belieben um mehr Kovariaten erweitern. Das sollte (wie gerade besprochen) auch passieren, da sie besonders dann gut geeignet ist, wenn auf viele relevante Störvariablen kontrolliert werden kann. Dabei gehen wir bereits etwas über die ursprüngliche Auswertung von [Schleider et al. (2022)](https://doi.org/10.1038/s41562-021-01235-0) hinaus, die nur die Baselinevariable als Kovariate genutzt hatten. Das Besondere an der ANCOVA ist darüber hinaus aber, dass wir sie in den Fall der _generalisierten ANCOVA_ erweitern können. 

Im vorherigen Abschnitt zur ANCOVA hatten wir angenommen, dass Kovariaten den Effekt eventuell verzerren könnten und sie daher aufgenommen, um einen "bereinigten" Effektschätzer der Intervention zu erhalten. Dabei wird allerdings (wie an den parallelen Linien zu sehen war) davon ausgegangen, dass der Interventionseffekt bei allen Ausprägungen der Kovariate der Gleiche ist. Diese Annahme lässt sich prüfen, indem wir einen Interaktionseffekt in die ANCOVA aufnehmen. Dafür gilt, wie schon im [Beitrag zur moderierten Regression](/lehre/klipps/moderierte-regression-klinische), dass wir kontinuierliche Variablen unbedingt zentrieren sollten. Das hilft zum Einen bei der Interpretaion, aber kann auch Multikollinearität verringern.

```{r}
# Zenrierung der Kovariate
cope$CDI1_c <- scale(cope$CDI1, scale = FALSE)

# Generalisierte ANCOVA
mod4 <- lm(CDI3 ~ CDI1_c * condition, cope)

# Ergebniszusammenfassung
summary(mod4)
```
Die Ergebnisse zeigen keine bedeutsamen Interaktionseffekte. Das bedeutet also, dass die Interventionen in ihrer Wirksamkeit nicht statistisch bedeutsam davon abhängen, mit welchem Depressionsscore die Personen die Studie gestartet haben. 

```{r ancova-generalized, echo = FALSE, fig = TRUE, warning = FALSE}
# ANCOVA mit zentriertem Prädiktor aufstellen (zum Vergleich)
mod2b <- lm(CDI3 ~ CDI1_c + condition, cope)

# Regressionsergebnisse zusammenführen
lines <- data.frame(condition = c("Placebo Control", "Project Personality", "Project ABC"), 
  int1 = coef(mod2b)[1] + c(0, coef(mod2b)[3:4]),
  slo1 = coef(mod2b)[2],
  int2 = coef(mod4)[1] + c(0, coef(mod4)[3:4]),
  slo2 = coef(mod4)[2] + c(0, coef(mod4)[5:6]))

# Grafische Darstellung
scatter <- ggplot(cope, aes(x = CDI1_c, y = CDI3, color = condition)) + 
  theme_pandar() + scale_color_pandar() + geom_point(alpha = 0) + xlim(-.25, .25) + ylim(.75, 1.25)

scatter +
  geom_abline(data = lines, aes(intercept = int1, slope = slo1, color = condition), linetype = 'solid') +
  geom_abline(data = lines, aes(intercept = int2, slope = slo2, color = condition), linetype = 'dashed') 

```

Erneut sind sowohl optisch als auch numerisch die Unterschiede zwischen den Interventionsansätzen minimal. Dennoch sehen wir, dass im Bereich der unterdurchschnittlichen `CDI1`-Werte Teilnehmende des Project ABC niedrigere Depressionswerte zum Follow-Up aufweisen, während sich dieser Effekt bei überdurchschnittlichen Werten zugunsten von Project Personality verschiebt. Das können wir erneut (wie schon bei der moderierten Regression) mit Simple Slopes testen:

```{r}
# Simple Slopes
library(interactions)
sim_slopes(mod4, pred = condition, modx = CDI1_c)
```
In Fällen, in denen solche Effekte deutlicher ausfallen, könnten wir diese Ergebnisse heranziehen, um z.B. für Patient\*innen mit hoher Belastung eine Intervention zu empfehlen, für Patient\*innen mit niedrigerer Belastung aber eine Andere.

***

## Literatur

Lord, F. M. (1967). A Paradox in the Interpretation of Group Comparisons. _Psychological Bulletin 68_(5), 304-305. https://doi.org/10.1037/h0025105.

Lüdtke, O. & Robitzsch, A. (2023). ANCOVA versus Change Score for the Analysis of Two-Wave Data. _The Journal of Experimental Education_, 1–33. https://doi.org/10.1080/00220973.2023.2246187

Schleider, J.L., Mullarkey, M.C., Fox, K.R., Dobias, M. L., Shroff, A., Hart, E. A. & Roulston, C. A. (2022). A randomized trial of online single-session interventions for adolescent depression during COVID-19. _Nature Human Behavior, 6_, 258–268. https://doi.org/10.1038/s41562-021-01235-0

van Breukelen, G. J. P. (2013). ANCOVA Versus CHANGE From Baseline in Nonrandomized Studies: The Difference. _Multivariate Behavioral Research, 48_(6), 895–922. https://doi.org/10.1080/00273171.2013.831743