---
title: "Matrixalgebra" 
type: post
date: '2025-01-15' 
slug: matrixalgebra
categories: ["Statistik I"] 
tags: ["Grundlagen", "Matrix"] 
subtitle: ''
summary: 'In dieser Sitzung werden die Grundlagen der Matrixalgebra erklärt und ihr Anwendungszweck für die Deskriptivstatistik ausgeführt. Die Grundlagen umfassen dabei Rechenoperationen auf Vektoren und Matrizen, sowie die Bestimmung von Determinanten und Inversen.' 
authors: [irmer, liu, sajjad] 
weight: 10.5
lastmod: '`r Sys.Date()`'
featured: no
banner:
  image: "/header/windmills_but_fancy.jpg"
  caption: "[Courtesy of pxhere](https://pxhere.com/en/photo/1178076)"
projects: []
reading_time: false
share: false

links:
  - icon_pack: fas
    icon: book
    name: Inhalte
    url: /lehre/statistik-i/matrixalgebra
  - icon_pack: fas
    icon: terminal
    name: Code
    url: /lehre/statistik-i/matrixalgebra.R
  - icon_pack: fas
    icon: pen-to-square
    name: Übungen
    url: /lehre/statistik-i/matrixalgebra-uebungen
output:
  html_document:
    keep_md: true
---

```{r setup, cache = FALSE, include = FALSE, purl = FALSE}
if (exists("figure_path")) {
  knitr::opts_chunk$set(fig.path = figure_path)
}
# Aktuell sollen die global options für die Kompilierung auf den default Einstellungen gelassen werden
```

***
<details><summary><b>Kernfragen dieser Lehreinheit</b></summary>

* Wie definiere ich [**Vektoren**](#Vektoren) und wie kann ich mit ihnen [**Rechenoperationen**](#VekRechOp) durchführen?  
* Wie definiere ich [**Matrizen**](#Matrizen) und wie kann ich mit ihnen [**Rechenoperationen**](#MatRechOp)  durchführen?
* Wie bestimme ich [**Determinante und Inverse**](#DetInv) einer Matrix?
* Wie nutze ich Matrixoperationen um [**Deskriptivstatistiken**](#Deskriptiv) zu bestimmen?

</details>

***

Bisher haben wir gelernt, dass viele statistische Größen mithilfe von Summen, Mittelwerten, Quadraten, Abweichungen und weiteren recht einfachen Rechenoperationen bestimmt werden können. Diese können oft noch einfacher mithilfe von Matrizen und Vektoren dargestellt werden. Vielleicht sind Ihnen Matrizen aus der Schule bekannt. Falls nicht, ist das kein Problem, denn dieser Beitrag wird Vektoren und Matrizen ausführlich erklären. Matrixalgebra wird bspw. auch in Eid et al. (2017) im _Anhang B: Matrixalgebra_ ab Seite 1051 behandelt.

Vektoren kennen wir bereits aus den ersten Datensätzen, die wir kennengelernt haben. Bspw. enthält ein Variablenvektor einer Person einfach nur die Einträge der Variablen aus dem Datensatz dieser spezifischen Person. Matrizen haben wir im Grunde auch schon kennengelernt. Ein Datensatz ist eng mit einer Matrix verwandt. Genauso wie ein Datensatz, besteht auch eine Matrix aus Zeilen und Spalten. Der Hauptunterschied ist, dass bei einer Matrix nur numerische Inhalte, also Zahlen, erlaubt sind. Wenn wir Daten in `R` verarbeiten wollen, wird der Datensatz oft in Matrizen umtransformiert (falls er vorher nicht-numerische Inhalte enthielt) und dann mit geeigneten Operationen, sogenannten Matrixoperationen, verarbeitet.

`R` ist eine vektorbasierte Programmiersprache, was bedeutet, dass möglichst viel mit Vektor- oder Matrixoperationen durchgeführt werden soll, da diese besonders optimiert (und damit besonders schnell) sind. Um davon Gebrauch zu machen, müssen wir uns mit diesen Operationen vertraut machen: 

#### Vektoren {#Vektoren}
Vektoren werden häufig (aber nicht immer, Ausnahmen bestätigen die Regel) in Kleinbuchstaben dargestellt. 
Seien `x` und `y` zwei Vektoren, die wir mit dem Zuordnungspfeil `<-` und mit der Vektorfunktion `c()` erstellen:

```{r, echo=-c(1,2)}
## Vektorrechnung

x <- c(4, 2, 3)
y <- c(10, 8, 6)
```

Vektoren werden meistens als sogenannte Spaltenvektoren (dazu später mehr) dargestellt. `x` und `y` sehen also so aus


$$x=\begin{pmatrix}4\\2\\3 \end{pmatrix}, \qquad y=\begin{pmatrix}10\\8\\6 \end{pmatrix}.$$


Die Elemente werden mit der jeweiligen Position im Vektor nummeriert. Das Element $x_1$ ist also das 1. Element von $x$, nämlich 4. Dies kann auch so dargestellt werden:


$$x=\begin{pmatrix}x_1\\x_2\\x_3 \end{pmatrix}=\begin{pmatrix}4\\2\\3 \end{pmatrix}, \qquad y=\begin{pmatrix}y_1\\y_2\\y_3 \end{pmatrix}=\begin{pmatrix}10\\8\\6 \end{pmatrix}.$$

***

<details><summary><b>Wiederholung: Datenextraktion bei Vektoren</b></summary>

Wir erkennen, dass den Elementen $x_1, x_2, x_3$ die Zahlen 4, 2, und 3 und den Elementen $y_1, y_2, y_3$ die Zahlen 10, 8, und 6 zugeordnet werden.


Wir können auf Elemente eines Vektors mit eckigen Klammern zugreifen. Bspw. erhalten wir das 2. Element von Y (also quasi $y_2$) mit

```{r}
y[2]
```

Auch mehrere Elemente lassen sich auf diese Weise ausgeben. Sind wir bspw. am 2. bis 3. Element interessiert, können wir 

```{r}
y[2:3]
```

schreiben. Interessiert uns hingegen das 1. und 3. Element, brauchen wir erneut einen Vektor, der die Position auswählt:

```{r}
y[c(1,3)]
```

</details>

***

##### Addition und Subtraktion bei Vektoren

Die Addition von Vektoren funktioniert elementenweise. Das bedeutet, dass das 1. Element des 1. Vektors und das 1. Element des 2. Vektors miteinander addiert werden und das 2. Element des 1. Vektors mit dem 2. Element des 2. Vektors miteinander addiert werden, etc. 

\begin{equation*}
\small
x+y=\begin{pmatrix}x_1\\x_2\\x_3 \end{pmatrix}+\begin{pmatrix}y_1\\y_2\\y_3 \end{pmatrix}=\begin{pmatrix}x_1+y_1\\x_2+y_2\\x_3+y_3 \end{pmatrix}=\begin{pmatrix}4\\2\\3 \end{pmatrix}+\begin{pmatrix}10\\8\\6 \end{pmatrix}=\begin{pmatrix}4+10\\2+8\\3+6 \end{pmatrix}=\begin{pmatrix}14\\10\\9 \end{pmatrix}.
\end{equation*}


Elementeweise Additionen funktionieren super simpel, indem wir `x` und `y` einfach mit `+` verknüpfen. 

```{r}
x + y  # Addition
```

Wenn `x` und `y` nicht dieselbe Länge haben, ist es in `R` oft so, dass die Vektoren künstlich verlängert werden, um verrechnet zu werden. Dies sollten wir immer im Hinterkopf behalten.

```{r}
z <- c(1:6) # Zahlen 1 bis 6
z + y
```

`z` ist hier doppelt so lang wie `y`, sodass in der Addition `y` einfach zweimal hintereinander geschrieben wird, damit die Addition möglich ist, denn eine Addition bei Vektoren (und auch Matrizen) funktioniert nur, wenn die beiden Elemente das gleiche Format haben! Im Kontext der Matrixalgebra ist die künstliche Verlängerung eines Vektors zur Addition kein zulässiger Rechenschritt, sondern eine Besonderheit von `R`, was hin und wieder zu Problemen oder Fehlern führen kann. Die Länge eines Vektors entspricht seiner Anzahl an Elementen. Diese können wir erhalten über

```{r}
length(x)
```


Analog zur Addition erfolgt auch die Subtraktion von Vektoren elementenweise. Wollen wir $y - x$ berechnen, so würden wir entsprechend zur Addition das 1. Element von $y$ verrechnen mit dem 1. Element von $x$ usw. und erhalten folgendes Resultat:

$$y-x=\begin{pmatrix} y_1\\ y_2\\ y_3 \end{pmatrix} - \begin{pmatrix}x_1\\ x_2\\ x_3 \end{pmatrix}=\begin{pmatrix}y_1-x_1\\ y_2-x_2\\ y_3-x_3 \end{pmatrix} =\begin{pmatrix}10-4\\ 8-2\\ 6-3 \end{pmatrix}=\begin{pmatrix}6\\ 6\\ 3 \end{pmatrix}.$$
```{r}
y-x
```

##### Multiplikation und Division bei Vektoren 

Bezüglich der Multiplikation und Division bei Vektoren wird unterschieden zwischen zwei Rechenoperationen: (1) Die Multiplikation bzw. Division eines Vektors mit einer Zahl, also einem Skalar, und (2) die Multiplikation zweier Vektoren. Zunächst befassen wir uns mit der ersten Variante. Wenn wir einen Vektor mit einer Zahl bzw. einem Skalar multiplizieren, so bewirkt dies eine elementenweise Multiplikation mit dieser Zahl. Wenn wir beispielsweise $k$ mit $x$ multiplizieren wollen, so erhalten wir:


$$kx=k\begin{pmatrix}x_1\\x_2\\x_3 \end{pmatrix}=\begin{pmatrix}kx_1\\kx_2\\kx_3 \end{pmatrix}.$$


Für $k=3$ erhalten wir bspw.


$$kx=3\begin{pmatrix}4\\2\\3 \end{pmatrix}=\begin{pmatrix}3\cdot 4\\3\cdot2\\3\cdot3 \end{pmatrix}=\begin{pmatrix}12\\6\\9 \end{pmatrix}.$$


In `R` sieht das so aus

```{r}
3*x
```

Genauso können wir auch jedes Element durch 2 teilen, indem wir den Vektor $x$ mit dem Skalar $\frac{1}{2}$ also 0.5 multiplizieren.

```{r}
1/2*x
```
Im Kontrast ist die zweite Rechenoperation, nämlich die Multiplikation zweier Vektoren, komplexer. An dieser Stelle werden wir das Mysterium noch nicht lüften, weil es sich anbietet, diese besondere Form der Multiplikation später bei den Matrizen im Detail zu behandeln. Wir möchten jedoch darauf verweisen, dass es in `R` möglich ist, zwei Vektoren der gleichen Länge miteinander elementenweise zu multiplizieren über folgenden Ausdruck:


```{r}
x*y 
```
Hierbei handelt es sich jedoch nicht um eine matrixalgebraische Multiplikation, sondern eine `R`-spezifische Art der Multiplikation. 

#### Matrizen {#Matrizen}

Ein Vektor ist eine eindimensionale Sammlung von Zahlen. Die Elemente werden einfach durchnummeriert. Eine Matrix ist ein zweidimensionales Objekt, welche aus einer Vielzahl von Vektoren gleicher Länge besteht, die aneinander "geklebt" werden. Matrizen werden oft in Großbuchstaben beschrieben. Elemente von Matrizen hingegen in Kleinbuchstaben. Auch hier ist das nicht wirklich einheitlich geregelt. 

Würden wir die beiden Vektoren $x$ und $y$ aneinander "kleben", erhalten wir eine Matrix $A$, welche 3 Zeilen und 2 Spalten enthält. 


$$A=\begin{pmatrix}a_{11} & a_{12}\\ a_{21} & a_{22}\\a_{31} & a_{32}  \end{pmatrix} = \begin{pmatrix} 4 & 10 \\ 2& 8 \\ 3 & 6 \end{pmatrix}$$
Analog zu Vektoren ist die Position in einer Matrix auch über Indizes definiert, wobei nun zwei Laufnummern vergeben werden für jedes Element $a_{ij}$. Der erste Index $i = 1,2,...,n$ bezieht sich auf die Zeile, in welchem sich das Element befindet und der zweite Index $j=1,...,m$ die Spalte. Demnach beschreibt $a_{32}$ das Matrixelement in der 3. Zeile (1. Index) und 2. Spalte (2. Index). Für die Matrix $A$ entspricht dies dem Wert 6. Das allgemeine Format einer Matrix kann über ihre Typangaben beschrieben werden: Auch hier wird zuerst die Anzahl der Zeilen der Matrix angegeben und dann ihre Spalten, sprich in der Form $z \times s$. Bei Matrix $A$ handelt es sich also um eine Matrix vom Typ $3 \times 2$ (3 Zeilen und 2 Spalten). Im Folgenden sind ein paar weitere Beispiele aufgelistet:


Typ $4\times2$: $\begin{pmatrix} 2 & 4 \\ 5 & 1 \\ -2 & 3\\ 1 & 2 \end{pmatrix}$

Typ $2\times3$: $\begin{pmatrix} 5 & 7 & 9 \\ 1 & 9 & 4\end{pmatrix}$

Typ $3\times1$ (sog. Spaltenvektor): $\begin{pmatrix} 4 \\ 2 \\ 3 \end{pmatrix}$

Typ $1\times3$ (sog. Zeilenvektor): $\begin{pmatrix} 2 & 5 & 7 \end{pmatrix}$

Die gerade behandelten Vektoren können wir ganz leicht zu einer Matrix machen, indem wir den Befehl `as.matrix` bspw. auf `x` anwenden. Dieser Befehl erzeugt  eine $3\times1$ Matrix - also aus mathematischer Sicht einen Spaltenvektor.

```{r, echo=-c(1,2)}
## Matrixrechnung

as.matrix(x)
```

Wir können die beiden Vektoren auch zu einer Matrix kombinieren, indem wir sie mit dem Befehl `cbind` (was für column binding steht) zusammenfügen - genauso geht dies auch mit `rbind` (was für row binding steht):

```{r}
A <- cbind(x, y)
A
B <- rbind(x, y)
B
```

Man erkennt, dass mit `cbind` die beiden Vektoren als Spaltenvektoren nebeneinander verknüpft werden, während bei `rbind` die Vektoren als Zeilenvektoren untereinander verknüpft wurden. 

***

<details><summary><b>Wiederholung: Datenextraktion bei Matrizen</b></summary>

Wir können nun bspw. den Eintrag $b_{12}$ herauslesen via `[1, 2]`, wobei der 1. Eintrag immer für die Zeile und der 2. für die Spalte steht:

```{r}
B[1, 2] 
```


Eine ganze Zeile oder Spalte erhalten wir, indem wir eines der Elemente in der Indizierung frei lassen:

```{r}
B[1, ] # 1. Zeile
B[, 2] # 2. Spalte
```


</details>

***

##### Transposition, Addition und Subtraktion

So wie `A` und `B` erzeugt wurden, ist ersichtlich, dass die Spalten von `A` den Zeilen von `B` entspricht. Wir können Zeilen und Spalten auch vertauschen, indem wir die Matrix mit dem Befehl `t()` transponieren:

```{r}
A
t(A)
B
```

Wir erkennen, dass die Matrix `B` gerade die Transponierte von `A` ist! Im Prinzip wurden die Indizes von jedem Element der Matrix getauscht, sprich durch die Transposition von $A$ wurde jedes Element $a_{ij}$ zu $a_{ji}$. 

Die Matrixaddition und Matrixsubtraktion funktioniert genauso wie die von Vektoren. Sie wird elementenweise durchgeführt. Allerdings müssen dafür die Matrizen vom selben Typ sein, also gleich viele Zeilen und Spalten besitzen. 

Die beiden Matrizen `A` und `B` lassen sich beispielsweise nicht addieren, da sie nicht das richtige Format haben:

```{r, eval = F}
A + B
```

```{r, echo = F, purl = F}
cat("Error in A + B : non-conformable arrays")
```

Eine $3\times 2$ Matrix lässt sich nicht mit einer $2\times 3$ Matrix addieren. Wird eine Matrix transponiert, so ändert sich damit auch ihr Typ. Die Zeilen werden zu Spalten und umgekehrt. Die Transponierte von $A$ ist also eine $2\times 3$ Matrix und lässt sich damit mit $B$ addieren:

```{r}
t(A) + B
```

Da die beiden Matrizen `t(A)` und `B` identisch sind, würden wir zum selben Ergebnis kommen, wenn wir jeden Eintrag von $B$ mit dem Skalar 2 multiplizieren.  

```{r}
B * 2 # skalare Multiplikation
```
 

##### Multiplikation zweier Matrizen

Wie bereits im Abschnitt zur [Multiplikation und Division bei Vektoren] angedeutet, funktioniert die Multiplikation von Vektoren, und als Erweiterung die Multiplikation von zwei Matrizen, nicht elementenweise. Stellen wir uns vor, wir möchten die beiden Matrizen $A$ und $B$ miteinander multiplizieren, also $AB$ rechnen. Man kann zwei Matrizen nur dann miteinander multiplizieren, wenn ihre inneren Typangaben miteinander korrespondieren, d.h. wenn die ,,erste’’ Matrix (also $A$) genauso viele Spalten wie die ,,zweite’’ Matrix (also $B$) Zeilen hat: $A B = (p \times q)  (q \times r) = (3 \times 2)  (2 \times 3)$. Da die Matrix $A$ zwei Spalten und die Matrix $B$ zwei Zeilen hat, können die beiden Matrizen miteinander multipliziert werden. 

Konkret werden bei der Matrixmultiplikation die Zeilen der ersten Matrix $A$ mit den Spalten der zweiten Matrix $B$ elementenweise multipliziert und diese Elemente werden anschließend addiert. Die resultierende Matrix hat dann so viele Zeilen wie die erste Matrix und so viele Spalten wie die zweite. Das resultierende Format der neuen Matrix ist also $p\times r$.


\begin{align*}
AB = \begin{pmatrix}a_{11} & a_{12}\\ a_{21} & a_{22}\\a_{31} & a_{32}\\ \end{pmatrix} \begin{pmatrix}b_{11} & b_{12} & b_{13}\\ b_{21} & b_{22} & b_{23} \end{pmatrix}\\
&= \begin{pmatrix} 
a_{11}b_{11}+a_{12}b_{21} &  
a_{11}b_{12}+a_{12}b_{22}&  
a_{11}b_{13}+a_{12}b_{23}\\ 
a_{21}b_{11}+a_{22}b_{21} & 
a_{21}b_{12}+a_{22}b_{22} & 
a_{21}b_{13}+a_{22}b_{23}\\
a_{31}b_{11}+a_{32}b_{21} & 
a_{31}b_{12}+a_{32}b_{22}& 
a_{31}b_{13}+a_{32}b_{23}\end{pmatrix}.
\end{align*}


Für unseren Fall entsteht eine neue Matrix der Dimension $3\times 3$. 

Drehen wir das Ganze um, erhalten wir

\begin{align*}
BA &=  \begin{pmatrix}b_{11} & b_{12} & b_{13}\\ b_{21} & b_{22} & b_{23} \end{pmatrix} \begin{pmatrix}a_{11} & a_{12}\\ a_{21} & a_{22}\\a_{31} & a_{32}\\ \end{pmatrix}\\
&= \begin{pmatrix} 
b_{11}a_{11}+b_{12}a_{21} + b_{13}a_{31} &  
b_{11}a_{12}+b_{12}a_{22} + b_{13}a_{32} \\ 
b_{21}a_{11}+b_{22}a_{21} + b_{23}a_{31} &  
b_{21}a_{12}+b_{22}a_{22} + b_{23}a_{32}  \end{pmatrix}.
\end{align*}


Hier entsteht eine Matrix vom Format $2\times 2$. Wir sehen deutlich, dass Matrixmultiplikation im Allgemeinen nicht kommutativ ist, also $AB \neq BA$.

Der Operator in `R` hierfür heißt `%*%` (verwenden wir stattdessen `*`, so wird eine elementenweise Multiplikation durchgeführt, was etwas komplett anderes ist!):

```{r}
A %*% B # Matrixprodukt A*B
B %*% A # Matrixprodukt B*A
```

An den Ergebnissen erkennen wir auch, dass Matrixprodukte nicht kommutativ sind, also die Reihenfolge, in der multipliziert wird, wichtig ist. Analog zur Multiplikation zweier Matrizen kann man zwei Vektoren miteinander multiplizieren. Einen Exkurs hierfür finden Sie im Appendix.


#### Spezielle Matrizen
Eine quadratische Matrix ist eine Matrix mit gleich vielen Zeilen wie Spalten. Eine wichtige quadratische Matrix ist die Einheitsmatrix $I$, welche nur 1en auf der Diagonalen und sonst 0en hat. Diese ist gerade das Element, mit welchem wir getrost multiplizieren können (falls die Dimensionen stimmen), weil dann nichts passiert (wie Multiplikation mit 1 bei Zahlen). Wir erhalten sie mit `diag`, was eigentlich eine (quadratische) Diagonalmatrix erzeugt mit beliebigen Elementen auf der Diagonalen:

```{r, echo=-c(1,2)}
## Spezielle Matrizen

diag(3) # Einheitsmatrix 3x3
diag(1:3) # Diagonalmatrix mit Elementen 1,2,3 auf der Diagonalen
```

Bisher haben wir Matrizen dadurch konstruiert, dass wir zwei Vektoren mit `cbind()` oder `rbind()` miteinander verknüpft haben. Wir können jedoch eine Matrix mit dem `matrix()`  direkt bestimmen. Diesem übergeben wir einen Vektor und die Dimensionen der Matrix. Dem Argument `data` werden die Daten, die wir in die Matrix schreiben wollen, übergeben, `nrow` und `ncol` bestimmen die Anzahl der Zeilen und Spalten und mit `byrow = T` legen wir fest, dass wir die Matrix zeilenweise gefüllt bekommen möchten:

```{r}
C <- matrix(data = c(1:9), # Daten/Inhalt des Vektors (hier die Zahlen 1 bis 9)
            nrow = 3,      # Zeilenanzahl der Matrix
            ncol = 3,      # Spaltenanzahl der Matrix
            byrow = T)     # Zeilen der resultierenden Matrix werden zuerst mit data gefüllt
C
```

Wir können mit `diag` auch wieder die Diagonalelemente einer Matrix erfahren:

```{r}
diag(C)
```


##### Determinanten und Invertierung {#DetInv}

Die Inverse, also jene Matrix, mit der wir eine Matrix $X$ multiplizieren müssen, um die Einheitsmatrix $I$ zu erhalten ($XX^{-1} = I$), lässt sich in `R` mit dem `solve` Befehl erhalten (dies geht nur bei quadratischen Matrizen):

```{r, eval = F, purl=FALSE}
solve(C)
```

```{r, echo = F, purl = F}
cat("Error in solve.default(C) : 
  system is computationally singular: reciprocal condition number = 2.59052e-18")
```

Obwohl $C$ eine quadratische Matrix ist, lässt sie sich nicht invertieren, da sie singulär und damit nicht invertierbar ist. Dies bedeutet, dass es lineare Abhängigkeiten der Zeilen bzw. Spalten gibt. Wir können dies explizit prüfen, indem wir die Determinante bestimmen mit `det`:

```{r, echo=-c(1,2)}
## Determinanten

det(C)
round(det(C), 14)
```
Mit `round` runden wir das Ergebnis auf die 14. Nachkommastelle. Eine Matrix ist genau dann invertierbar (also regulär im Vergleich zu singulär), wenn die Determinante dieser (quadratischen) Matrix **nicht Null** ist. Lineare Abhängigkeit bedeutet, dass die Zeilen oder Spalten durch Addition, Subtraktion und skalare Multiplikationen auseinander hervorgehen. Die lineare Abhängigkeit zwischen den Spalten wird ersichtlich, wenn wir von der 2. Spalte die 1. Spalte abziehen und das Ergebnis zur 3. Spalte addieren - also de facto $2*2.Spalte - 1. Spalte$  rechnen:

```{r}
2*C[, 2] - C[, 1]     # 2*2.Spalte - 1. Spalte rechnen ist gleich
C[, 3]               # 3. Spalte
```

Hätten wir `C^-1` gerechnet, so hätten wir eine elementeweise Invertierung durchgeführt:

```{r}
C^-1
C^-1 %*% C # ist nicht die Einheitsmatrix
C^-1 * C   # elementeweise ergibt überall 1 - ist immer noch nicht die Einheitsmatrix!
```

Dies bedeutet, dass `C^-1` in `R` nicht die Invertierung betitelt, sondern `solve`!

Betrachten wir nun eine invertierbare Matrix `D`:

```{r}
D <- matrix(c(1, 0, 0,
              1, 1, 1,
              2, 4, 5), 3, 3, byrow = T)
det(D)
```

Die Determinante von `D` ist 1. Somit können wir `D` invertieren. Das Produkt aus `D` mit seiner Inversen ergibt gerade die 3x3 Einheitsmatrix:

```{r}
solve(D)
D %*% solve(D)
solve(D) %*% D
```

Das Produkt von $D$ und $D^{-1}$ ist (ausnahmsweise) kommutativ: $DD^{-1}=D^{-1}D=I$. Im Allgemeinen ist die Bestimmung der Inversen einer Matrix komplex. Für eine $2 \times 2$-Matrix gibt es jedoch eine einfache Lösung. Eine $2\times 2$ Matrix 
$$M=\begin{pmatrix}a&b\\ c&d \end{pmatrix}$$ 
lässt sich genau dann invertieren, wenn die Determinante nicht 0 ist, also wenn keine lineare Abhängigkeit innerhalb der Zeilen oder der Spalten besteht. Die Determinante von $M$ bestimmen wir als 
$$\text{det}[M] = ad-bc.$$
Ist diese nicht 0, so erhalten wir die Inverse von $M$ durch
$$M^{-1}=\frac{1}{\text{det}[M]}\begin{pmatrix}d&-b\\ -c&a\end{pmatrix}=\frac{1}{ad-bc}\begin{pmatrix}d&-b\\ -c&a\end{pmatrix}.$$
Wir müssen also die Diagonalelemente von $M$ vertauschen und die Nebendiagonalelemente mit einem Minus versehen. Anschließend müssen wir jeden Eintrag durch die Determinante teilen (oder wir nehmen einfach `solve`).

```{r}
M <- matrix(c(2, 2, 3, 4), ncol = 2, nrow = 2, byrow = TRUE)
M
det(M)
M[1,1]*M[2,2] - M[1,2]*M[2,1] # Determinante mit Hand
K <- matrix(c(M[2,2], -M[1,2], M[2,1], M[1,1]), byrow = TRUE, ncol = 2, nrow = 2)
K # Kofaktorenmatrix
1/(M[1,1]*M[2,2] - M[1,2]*M[2,1])*K # Inverse von M
solve(M) # Inverse von M
```

Wofür sind Matrixoperationen in der Statistik wichtig? Im letzten Abschnitt schauen wir uns für `fb25` an, wie man einfache Statistiken mit Hilfe der Matrixalgebra bestimmen kann.




##### Statistiken mit Matrixoperationen bestimmen {#Deskriptiv}

```{r, echo=-c(1,2)}
#### Statistiken mit Matrixoperationen bestimmen ----

# Daten laden
load(url('https://pandar.netlify.app/daten/fb25.rda'))

```

Summen lassen sich sehr leicht auch durch ein Matrixprodukt darstellen. Nehmen wir beispielsweise die Big Five Variablen des `fb25` Datensatzes her und nennen diese `X`. Wenn wir einen Zeilenvektor der Dimension $1\times n$ bilden, wobei $n$ die Anzahl an Zeilen von `X` ist, dann können wir die Summe der Elemente pro Spalte von `X` durch ein Matrixprodukt ausdrücken:

```{r}
X <- as.matrix(fb25[, c("extra", "vertr", "gewis", "neuro", "offen")])
X <- na.omit(X)  # fehlende Werte wurden entfernt
n <- nrow(X)
z <- t(rep(1, n)) # 1en-Zeilenvektor der Länge n
z %*% X
colSums(X)
```
In der Vorlesung haben Sie auch folgende Formel zur Berechnung der Kovarianzmatrix für eine beliebige Anzahl an Variablen kennengelernt

$$\Sigma = \frac{1}{n-1} (X-\overline{X})'(X-\overline{X}),$$
wobei $X$ eine Datenmatrix, $\overline{X}$ eine Matrix mit den Mittelwerten der Variablen und $n$ die Stichprobengröße ist. Für unsere Matrix $X$ müssten Sie, würden Sie die Formel nicht anwenden, individuelle Varianzen und Kovarianzen berechnen, was bei fünf Variablen insgesamt 15 Befehlen in `R` entsprechen würde. Glücklicherweise kann man in `R` mit `cov` die Kovarianzmatrix erhalten, eine manuelle Durchführung ist demnach nicht erforderlich: 

```{r}
cov(X) 
```

Der Vollständigkeit halber werden wir aber beispielhaft die Kovarianzmatrix mit Hilfe der obigen Formel manuell berechnen, um auch zu prüfen, ob `cov` tatsächlich richtige Lösungen ausgibt. Im ersten Schritt benötigen wir die Mittelwertmatrix $\overline{X}$, welche wir mit folgenden Formeln, die Sie auch in der Vorlesung kennengelernt haben, erhalten

$$\overline{x}' = \frac{1}{n}u'X, \qquad \overline{X} = u\overline{x}',$$
wobei $\overline{x}'$ ein Vektor der Mittelwerte und $u'$ ein Vektor bestehend aus den Werten 1 ist. Für unsere Matrix $X$ erhalten wir die Größen in `R` folgendermaßen: 

```{r}
u <- rep(1, nrow(X)) # u enthält so viele Einsen als Elemente, wie die Matrix X an Beobachtungen hat
x_mean <- 1/nrow(X) * t(u) %*% X
x_mean # Mittelwertevektor

```
```{r}
X_mean <- u %*% x_mean # Mittelwertematrix
head(X_mean) # um die ersten sechs Zeilen der Matrix anzuzeigen
```


Nun können wir die Differenz zwischen der Datenmatrix $X$ und seiner Mittelwertmatrix $\overline{X}$ berechnen ($X-\overline{X}$). Wir führen also eine Zentrierung durch, da wir lediglich den Mittelwert von jedem beobachteten Wert abziehen.

```{r}
X_zentriert <- X - X_mean
head(X_zentriert)
```
Zuletzt können wir nun das Kreuzprodukt $(X-\overline{X})'(X-\overline{X})$ berechnen und mit $1/(n-1)$ multiplizieren: 

```{r}
kreuzprodukt <- t(X_zentriert) %*% X_zentriert
X_cov <- 1/(nrow(X)-1) * kreuzprodukt
X_cov
```
Nicht überraschend kommen wir zu demselben Ergebnis wie `cov`. Mit Hilfe der Matrixalgebra besteht also die Möglichkeit, Statistiken wie die Kovarianzen und Varianzen über eine Formel direkt zu berechnen, statt sie individuell zu berechnen, was insbesondere bei einer hohen Anzahl an Variablen aufwendig wäre. 

***
<details><summary><b>Exkurs: Multiplikation zweier Vektoren</b></summary>

Analog zur Multiplikation zweier Matrizen müssen bei Vektoren die inneren Typangaben miteinander korrespondieren, sprich der erste Vektor muss so viele Spalten haben wie der zweite Vektor an Zeilen. Stellen wir uns vor, wir möchten folgende beide Vektoren $x$ und $y$ miteinander multiplizieren: 

$$x = \begin{pmatrix} 3 & -2 & 1\end{pmatrix}, \qquad y = \begin{pmatrix} 5 \\ 1 \\ 7 \end{pmatrix} $$ 
Demnach ist eine Multiplikation $xy$ möglich, da der Zeilenvektor $x$ genauso viele Spalten hat, wie der Spaltenvektor $y$ an Zeilen, nämlich jeweils 3 [$xy = (1 \times 3)(3 \times 1)$]. Hieraus können wir bereits ableiten, dass die resultierende Matrix aus der Multiplikation dem Typ $1 \times 1$ entsprechen wird. Hierbei spricht man auch von einer Skalarmultiplikation, weil das Produkt der zwei Vektoren eine Matrix mit nur einem Element ist (nicht zu vertauschen mit der Multiplikation eines Vektors mit einem Skalar). 

Für die eigentliche Multiplikation wird jedes Element von $x$ mit dem korrespondierenden Element $y$ multipliziert. Anschließend werden die Produkte aufaddiert. Für unser Beispiel resultiert folgendes Ergebnis: 

$$xy = \begin{pmatrix} 3 & -2 & 1\end{pmatrix} \cdot \begin{pmatrix} 5 \\ 1 \\ 7 \end{pmatrix} = 3 \cdot 5 + (-2) \cdot 1 + 1\cdot 7 = 20 $$ 
In `R` kann die Skalarmultiplikation mit demselben Operator `%*%` durchgeführt werden.

```{r}
x <- matrix(data = c(3, -2, 1), nrow = 1) # Zeilenvektor x
y <- matrix(data = c(5, 1, 7), ncol = 1) # Spaltenvektor y

x %*% y
```

Die Multiplikation $yx$ würde in unserem Fall auch funktionieren, weil die inneren Typangaben immer noch miteinander korrespondieren würden [$xy = (3 \times 1)(1 \times 3)$], jedoch entspricht die resultierende Matrix dem Typ $3 \times 3$. 

```{r}
y %*% x
```
Demnach ist auch die Multiplikation zweier Vektoren nicht kommutativ, es gilt $xy \neq yx$.

</details>

***
