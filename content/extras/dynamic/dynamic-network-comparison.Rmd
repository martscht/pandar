---
title: "Dynamic Network Comparison" 
type: post
date: '2025-12-22'
slug: dynamic-network-comparison 
categories: ["DYNAMIC"] 
tags: [] 
subtitle: ''
summary: 'This page contains a collection of frequently asked questions about the comparison of dynamic network models. The questions were collected as part of the methodological consultation within the DYNAMIC project.' 
authors: [siepe] 
weight: 2
lastmod: "`r format(Sys.Date(), '%Y-%m-%d')`"
featured: no
banner: 
  image: "/header/lightbeams_converging_night.jpg"
  caption: "[Courtesy of pxhere](https://pxhere.com/en/photo/724752)"
projects: []
reading_time: false
share: false

links:
  - icon_pack: fas
    icon: book
    name: Inhalte
    url: /extras/dynamic/dynamic-network-comparison/
  - icon_pack: fas
    icon: terminal
    name: Code
    url: /extras/dynamic/dynamic-network-comparison.R
output:
  html_document:
    keep_md: true
---


```{r setup, cache = FALSE, include = FALSE, purl = FALSE}
if (exists("figure_path")) {
  knitr::opts_chunk$set(fig.path = figure_path)
}
# Aktuell sollen die global options für die Kompilierung auf den default Einstellungen gelassen werden
```

## How can dynamic network models be compared across individuals or groups?


When working with dynamic network models, researchers may be interested in comparing these models across individuals or groups. For example, one may want to test if the network structure differs between two clinical groups. Such comparisons have received great attention in cross-sectional network models (see, for example, Haslbeck, 2022). Recently, several methods have been proposed to compare dynamic network models. 
When working with person-specific models, researchers may also want to test if the estimated network structure is different after an intervention. For example, one may want to test if a dynamic network is less strongly connected after an intervention. 

Below, we outline some potential approaches to compare dynamic network models in these contexts. We differentiate between two scenarios: (1) comparing person-specific (n = 1) network models, and (2) comparing network models across multiple individuals or groups (n > 1). We provide simple example code snippets in R for each approach, which we typically borrow from the respective package documentations.

In general, many potential models can be portrayed and interpreted as dynamic networks. For example, one could fit a dynamic structural equation model (DSEM) and extract the parameters to create a network representation. Similarly, one could fit a continuous-time model and extract parameters to create a network representation. In the case of DSEM in particular, traditional SEM model comparison approaches can be used to compare models across individuals or groups. Therefore, the above approaches are not exhaustive, and researchers should carefully consider which is most suitable for their research question and data.


## How can n = 1 dynamic network models be compared?

For person-specific network models, there are several methods available to compare network structures before and after an intervention (for a given individual) or between individuals. Below, we outline four different approaches to compare n = 1 dynamic network models.

Typically, these models require many time points (e.g., 100 or more) to obtain reliable estimates of the dynamic network structure. However, there are no strict rules on the minimum number of time points required, as this depends on various factors such as the number of variables in the model, the expected effect sizes, and the estimation method used. For sample size planning, we recommend to take a look at simulation studies that have investigated the performance of the specific estimation method you plan to use. Alternatively, Zhang et al. (2025) have proposed simulation-based sample size planning approaches for person-specific network models. 



### Individual Network Invariance Test (INIT)

The Individual Network Invariance Test (INIT; Hoekstra et al., 2024) uses techniques from structural equation modeling to formally test whether two dynamic network models differ significantly from each other. To do so, a model that constrains all parameters to be equal across two networks is compared to a model that allows all parameters to differ across two networks. It can do so for temporal and contemporaneous networks simultaneously or separately.

Here is a simple example taken from the package documentation of the `INIT` package on [GitHub](https://github.com/RiaHoekstra/INIT):

```{r}
# If necessary, install the package
# install.packages("remotes")
# remotes::install_github("RiaHoekstra/INIT")

# Load the INIT package
library(INIT)

# Load data of two individuals: 
data <- INIT::data

# Compare networks with default arguments:
INIT(data = data, idvar = colnames(data)[1], vars = colnames(data)[2:7])
```

A significant results indicates that the networks are not invariant, that is, they differ significantly from each other. This test was used, for example, in Ebrahimi et al. (2024) to compare patients with major depressive disorder with similar severity levels.

### Invariance Partial Pruning (IVPP)

The Invariance Partial Pruning (IVPP; Du et al., 2025) is another approach to compare networks that is based on the idea of the INIT approach. However, instead of being restricted to a global test of invariance, it uses a stepwise procedure to identify if networks differ, and if so, which specific parameters differ across networks. It works for individual dynamic network models (n = 1) as well as for panel dynamic network models (n > 1).

Here is a simple example taken from the package documentation of the `ivpp` package on [CRAN](https://doi.org/10.32614/CRAN.package.ivpp):


```{r}
# If necessary, install the package
# install.packages("IVPP")

library(IVPP)

# Generate an n = 1 GVAR network
net_ls <- gen_tsGVAR(n_node = 6,
                     p_rewire_temp = 0.5,
                     p_rewire_cont = 0.5,
                     n_persons = 2)

# net_ls$beta contains the temporal networks for both persons
# net_ls$kappa contains the contemporaneous networks for both persons

# Generate the data for 300 timepoints
data <- sim_tsGVAR(beta_base_ls = net_ls$beta, 
                   kappa_base_ls = net_ls$kappa,
                   n_time = 300)

# global test on both networks
omnibus_both <- IVPP_tsgvar(data,
                            vars = paste0("V",1:6),
                            idvar = "id",
                            g_test_net = "both",
                            net_type = "sparse",
                            partial_prune = FALSE)

# partial prune on both networks
pp_both <- IVPP_tsgvar(data,
                       vars = paste0("V",1:6),
                       idvar = "id",
                       global = FALSE,
                       partial_prune = TRUE,
                       prune_net = "both")                             

``` 

The results indicate if networks differ, and if so, which specific parameters differ across networks.

### Bayesian Approach

Siepe et al. (2024) proposed a Bayesian approach to compare dynamic network models for a single individual. It works by estimating the posterior distribution of two dynamic network models and comparing these distributions to reference distributions that merely reflect sampling variability. 

Again, here is an example taken from the package documentation of the `tsnet` package on [CRAN](https://doi.org/10.32614/CRAN.package.tsnet):

```{r}
# If necessary, install the package
# install.packages("tsnet")

library(tsnet)

# Load simulated time series data of two individuals
data(ts_data)
data_1 <- subset(ts_data, id == "ID1")
data_2 <- subset(ts_data, id == "ID2")

# Estimate networks
# (You should perform preprocessing/detrending etc. in a real use case)
# This takes a while to run
net_1 <- stan_gvar(data_1[,-7],
                   iter_sampling = 4000,
                   n_chains = 4)
net_2 <- stan_gvar(data_2[,-7],
                   iter_sampling = 4000,
                   n_chains = 4)

# Compare networks
compare_13 <- compare_gvar(net_1, 
                           net_2,
                           return_all = TRUE,
                           n_draws = 1000)

# Print summary of results
print(compare_13)                   

```

A positive result indicates that it's unlikely that the two networks were generated from the same underlying parameters.

### Simple Permutation Test
Haslbeck (2025) proposed a simple test to compare two n = 1 dynamic network models, which is implemented in the `mnet` package. The test works by estimating a simple VAR model, and then either sampling from a reference distribution without differences, or by using a parametric test to compare the estimated parameters across two models. 

Here is an example based on the package documentation of `mnet` on [CRAN](https://doi.org/10.32614/CRAN.package.mnet).

```{r}
# If necessary, install the packages
# install.package(mlVAR)
# install.package(mnet)

library(mlVAR) # for simulateVAR() function
library(mnet)

# Specify simple VAR model
p <- 4
A1 <- diag(p) * 0.8
A2 <- diag(p) * 0.8
A2[2,1] <- 0.7

# Simulate datasets
Nt <- 300
set.seed(13) # for reproducibility
data1_x <- simulateVAR(A1, means=rep(0, p), Nt = Nt, residuals=.1)
data2_x <- simulateVAR(A2, means=rep(0, p), Nt = Nt, residuals=.1)

# Add beep and day vars
dayvar1 <- dayvar2 <- rep(1:(Nt/5), each=5)
beepvar1 <- beepvar2 <- rep(1:5, Nt/5)

# Add grouping var
groups1 <- rep(1, Nt)
groups2 <- rep(2, Nt)

# Combine
data1 <- data.frame(cbind(dayvar1, beepvar1, groups1, data1_x))
data2 <- data.frame(cbind(dayvar2, beepvar2, groups2, data2_x))
colnames(data1) <- colnames(data2) <- c("dayvar", "beepvar", "groups", paste0("V", 1:4))
data <- rbind(data1, data2)

# Call
out <- VAR_GC(data = data,
vars = 4:7,
dayvar = 1,
beepvar = 2,
groups = 3)
```

Significant results indicate that parameters differ across the two networks.

## How can n > 1 dynamic network models be compared?

When comparing group-based models, the number of timepoints per individual can often be lower (e.g., 50-100 time points), as information is pooled across individuals within each group. However, the exact number of time points required still depends on various factors such as the number of variables in the model, the expected effect sizes, the number of individuals per group, and the estimation method used. However, there should be a sufficient number of individuals per group to obtain reliable estimates of the differences between groups.
As explained above, the IVPP approach can also be used for panel network models (n > 1). Below, we outline an additional approach to compare dynamic network models across multiple individuals or groups.


### Permutation Test for mlVAR Models

Here, we again use the `mnet` package to perform a permutation test to compare mlVAR models across groups. It works by permuting group labels and estimating the group differences in parameters under the null hypothesis of no group differences.

```{r}
# If necessary, install the package
# install.package(mnet)

library(mnet)

# Use simulated example data loaded with package
head(ExampleData)

# Call Permutation test
out <- mlVAR_GC(data = ExampleData,
vars = c("V1", "V2", "V3"),
idvar = "id",
groups = "group",
nCores = 2, # choose max cores possible on your machine
nP = 2) # Should be much more in practice, see paper!

# P-values for the five parameter types:
out$Pval

# The observed group differences (i.e., the test statistics)
# Can be found in:
out$EmpDiffs

```

Significant results indicate that parameters differ across groups.

# References

Du, X., Johnson, S. U., & Epskamp, S. (2025). The Invariance Partial Pruning Approach to The Network Comparison in Time-Series and Panel Data. PsyArXiv Preprint. https://doi.org/10.31234/osf.io/vb8dz_v1

Ebrahimi, O. V., Borsboom, D., Hoekstra, R. H. A., Epskamp, S., Ostinelli, E. G., Bastiaansen, J. A., & Cipriani, A. (2024). Towards precision in the diagnostic profiling of patients: leveraging symptom dynamics as a clinical characterisation dimension in the assessment of major depressive disorder. *The British Journal of Psychiatry, 224*(5), 157–163. [https://doi.org/10.1192/bjp.2024.19](https://doi.org/10.1192/bjp.2024.19) 

Haslbeck, J. M. (2022). Estimating group differences in network models using moderation analysis. *Behavior Research Methods, 54*(1), 522-540. [https://doi.org/10.3758/s13428-021-01637-y](https://doi.org/10.3758/s13428-021-01637-y)

Haslbeck, J. (2025). mnet: Modeling group differences and moderation effects in statistical network models (Version 0.1.4) [R package]. Comprehensive R Archive Network (CRAN). https://doi.org/10.32614/CRAN.package.mnet

Hoekstra, R. H. A., Epskamp, S., Nierenberg, A. A., Borsboom, D., & McNally, R. J. (2024). Testing similarity in longitudinal networks: The Individual Network Invariance Test. *Psychological methods*. Advance online publication. [https://doi.org/10.1037/met0000638](https://doi.org/10.1037/met0000638)

Siepe, B. S., Kloft, M., & Heck, D. W. (2024). Bayesian estimation and comparison of idiographic network models. *Psychological methods*. Advance online publication. [https://doi.org/10.1037/met0000672](https://doi.org/10.1037/met0000672)

Zhang, Y., Revol, J., Lafit, G., Ernst, A. F., Razum, J., Ceulemans, E., & Bringmann, L. F. (2025). Meeting the Bare Minimum: Quality Assessment of Idiographic Temporal Networks Using Power Analysis and Predictive-Accuracy Analysis. *Advances in Methods and Practices in Psychological Science, 8*(4). [https://doi.org/10.1177/25152459251372116](https://doi.org/10.1177/25152459251372116)

